{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-class-categorization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIhTyf2xR07gsJAMKMgA8z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelkhan3/tensorflow_cert_checklist/blob/master/01-build-and-train-models/multi_class_categorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u2KlZGz38VN",
        "colab_type": "text"
      },
      "source": [
        "# Multi-class categorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HbeV0J14WMZ",
        "colab_type": "text"
      },
      "source": [
        "## Get data\n",
        "\n",
        "In this multi-class categorization exercise we will use the [Fashion-MINST dataset](https://github.com/zalandoresearch/fashion-mnist). Created by [Zalando](https://www.zalando.com/), this dataset contains images of 10 different types of fashion garnments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUk1A4wH36hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yRootOy97yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdPhk782_wY4",
        "colab_type": "text"
      },
      "source": [
        "Let's have a look at one of the images so we can see what they look like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4U4AvaL-yc9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "46e777b2-7162-46bb-f782-64df7dff8ef0"
      },
      "source": [
        "plt.imshow(x_train[33])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc58a194ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ir0Y8_5QJQC",
        "colab_type": "text"
      },
      "source": [
        "## Pre process images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCjvjfNgSfcF",
        "colab_type": "text"
      },
      "source": [
        "### Rescale images\n",
        "\n",
        "Rescale the pixel values to an int between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt2HUQ0aPM_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK66ECYjTu7H",
        "colab_type": "text"
      },
      "source": [
        "### Add a third dimention to the dataset\n",
        "\n",
        "Because our images are in greyscale they only have two dimentions (they don't have the three color channels). To feed the images into the `Conv2D` layer it needs to have a third dimentions, so we expand the train and test images with an extra dimention.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEi65Zk2VOX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b18f8b53-d624-4053-fb49-6e0116a696c8"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2EdXv0FVqQt",
        "colab_type": "text"
      },
      "source": [
        "`x_test` has 3 dimentions __(images_in_test_set, width_in_pixels, height_in_pixels)__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FklkdEhgTtsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UXdYwu1SShC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ab5be51-fceb-4a89-8e2d-c6d9a84095eb"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQzg3E4JWggi",
        "colab_type": "text"
      },
      "source": [
        "We have now added a 4th dimention for the grey color channel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zROVR9bASDR",
        "colab_type": "text"
      },
      "source": [
        "## Multi-class categorization model\n",
        "\n",
        "There are three important steps to make a model a __multi-class categirization__ model:\n",
        "\n",
        "### 1. The last layer of the model\n",
        "\n",
        "The job of the last layer of the model is to convert all the calculations of the neural network into the output you are looking for, in this case which garment is this picture of.\n",
        "\n",
        "For multi-class categorization the last layer should have one neuron for each class you are trying to categorize. In this case we have 10 different types of garnments so we use 10 neurons with softmax activation.\n",
        "\n",
        "One of the neurons will have an output that is close to 1, the rest of the neurons will have outpus that are close to 0\n",
        "\n",
        "![multi-class_classification.png](https://drive.google.com/uc?export=view&id=1KW1poPZkjGiONtsxUiqdzgrbUZvE-xsX)\n",
        "\n",
        "\n",
        "### 2. Use the correct loss function\n",
        "\n",
        "There are two options of loss functions for multi-class categorization:\n",
        "\n",
        "*   __Sparse categorical crossentropy__ - `tf.keras.losses.SparseCategoricalCrossentropy()`\n",
        "  - Use if the labels are integers e.g. `2`\n",
        "*   __Categorical crossentropy__ - `tf.keras.losses.CategoricalCrossentropy()` \n",
        "  - Use if the labels are one-hot encoded lists e.g. `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`\n",
        "\n",
        "\n",
        "### 3. Data labels are in the correct format\n",
        "\n",
        "Make sure the labels for your data are in the correct format. As mentioned above use the correct loss for the shape of your labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a2r7R26Wrk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6aa64f39-ce07-4544-cad2-477d5709c5bd"
      },
      "source": [
        "print(f\"The shape of training labels are {y_train.shape}\")\n",
        "print(f\"The first 10 labels = {y_train[:10]}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of training labels are (60000,)\n",
            "The first 10 labels = [9 0 0 3 0 2 7 2 5 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8Rj0vZ1aGtB",
        "colab_type": "text"
      },
      "source": [
        "Our labels are represented as integers, not one-hot encoded arrays, therefore we should use __sparse categorical crossentropy loss__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vjz0AHH_fwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_class_categorization_model = tf.keras.Sequential([\n",
        "                                                        tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "                                                        tf.keras.layers.MaxPool2D(2, 2),\n",
        "                                                        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                                                        tf.keras.layers.MaxPool2D(2, 2),\n",
        "                                                        tf.keras.layers.Flatten(),\n",
        "                                                        tf.keras.layers.Dense(256, activation='relu'),\n",
        "                                                        # Final layer has 10 neurons with sofmax activation\n",
        "                                                        tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH8ia6_vEA7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multi_class_categorization_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    # Use sparse categorical cross entropy loss for multi-class categorization\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvvovJAaHQsa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6e812930-528e-4375-bc2a-5a8be4479bc9"
      },
      "source": [
        "history = multi_class_categorization_model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.4551 - accuracy: 0.8356 - val_loss: 0.3525 - val_accuracy: 0.8712\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3014 - accuracy: 0.8892 - val_loss: 0.3100 - val_accuracy: 0.8856\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2572 - accuracy: 0.9054 - val_loss: 0.2913 - val_accuracy: 0.8918\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2248 - accuracy: 0.9155 - val_loss: 0.2639 - val_accuracy: 0.9041\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1965 - accuracy: 0.9265 - val_loss: 0.2571 - val_accuracy: 0.9055\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1722 - accuracy: 0.9356 - val_loss: 0.2542 - val_accuracy: 0.9096\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1512 - accuracy: 0.9432 - val_loss: 0.2654 - val_accuracy: 0.9090\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1304 - accuracy: 0.9510 - val_loss: 0.2646 - val_accuracy: 0.9129\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1124 - accuracy: 0.9574 - val_loss: 0.2819 - val_accuracy: 0.9093\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0976 - accuracy: 0.9634 - val_loss: 0.2991 - val_accuracy: 0.9055\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}